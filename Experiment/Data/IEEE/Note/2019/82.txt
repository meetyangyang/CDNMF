This paper is motivated by the exploration of the mobile robots remotely controlled by the disables inan unstructured environment that includes some unknown moving objects in the background. In the conventional approaches,the image feedback and the brainâ€“computer interface-based EEG signals invoked by the visual stimulus are used to guide themotion of the robots. In this paper, the MI of left or right handis adopted as an input of the brain teleoperation system, and theEEG signals are classified into two categories representing tworobot control commands through the designed algorithm. Besides,a deep-learning-based SLAM is designed to analyze the imageand depth of the environmental information provided by therobot RGB-D sensor, and the results can be transferred into a 3-Dmap locating the robot and assisting the operator to understandthe operating environment. It has been verified that the deeplearning-based SLAM presented is more efficient and robust thantraditional SLAM. The feasibility of the system is demonstratedby a set of experiments in the corridor environment.
