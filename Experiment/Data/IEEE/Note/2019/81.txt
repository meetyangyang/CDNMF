Heterogeneous sensors are increasinglyused to capture different aspects of an event or an environment.This paper presents a multimodal fusion mechanism to fuse suchheterogeneous sensor data to support the accurate perceptionof the surrounding. The proposed mechanism uses naturalregularities in patterns observed across multiple modalities andenriches each modality with information from other co-occurringmodalities to achieve a multimodal clustering. The cluster canbe used for online prediction and offline action planning of therobot by matching the current sensory inputs. The algorithm usesunsupervised machine learning, and hence, can be used with anymultimodal dataset without being restricted by the labeling ofthe dataset. In addition, we adapt the algorithm for multiple bigdata platforms so that it is effective and pragmatic for large-scalesensor data fusion tasks.
