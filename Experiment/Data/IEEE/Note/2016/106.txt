Learning grasp affordances for autonomous agents such as personal robots is a challenging task. We propose an unified framework which first detects target objects, infers grasp affordance of the target object, and executes robotic grasp. Our method is mainly based on 2-D imagery data which can be more robust when 3-D scans are unavailable due to background clutter and material properties such as surface reflectance. One of the future extensions would be to automate the training phase so that robots can actively learn object models by interacting with objects as opposed to having a human in the loop collecting and annotating training images.
