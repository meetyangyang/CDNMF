The goal of this work is to find a nearoptimal gantry assignment policy to realize real-time control ofmaterial handling gantry/robot movements in gantry work cells.Properly assigning gantries based on real-time situations of theproduction system can avoid machinesâ€™ stoppage due to materialshortage, and consequently improve production performance.This gantry scheduling is a sequential decision-making problemand can be presented by Markov Decision Process (MDP).To solve the MDP problem, an algorithm integrating model-freeQ-learning and model-based approximate dynamic programming(ADP) is proposed. By learning directly from the interaction withthe environment, the method avoids bias problem from any modeldesigning. Meanwhile, a planning process during learning canefficiently speed up the learning for convergence of the policy,and this particularly benefits to the scenario when the real dataare insufficient.
