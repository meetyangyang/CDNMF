This article was motivated by challengesin recognition tasks for dynamic and varying scenarios. Ourapproach learns to recognize new user interactions and objects.To do so, we use multimodal data from the userâ€“robot interaction;visual data are used to learn the objects and speech is used tolearn the label and help with the interaction-type recognition.We use state-of-the-art deep learning (DL) models to segment theuser and the objects in the scene. Our algorithm for incrementallearning is based on a classic incremental clustering approach.The pipeline we propose works with all sensors mounted on therobot, so it allows mobility on the system. This article uses thedata recorded from a Baxter robot, which enables the use ofthe manipulation arms in future steps, but it would work withany robot that can have the same sensors mounted. The sensorsused are two RGB-D cameras and a microphone. The pipelinecurrently has high computational requirements to run the twoDL-based steps. We have tested it with a desktop computer,including a GTX 1060 and 32 GB of RAM.
