This paper addresses the problem of inferring the goal location of a human hand motion observed using an RGB-D sensor while performing reaching tasks, such as picking up objects from a table. A Microsoft Kinect (3-D camera) sensor is used to track the joints of the human skeleton. The dynamics of the human arm motion are learned from the demonstrations of a human reaching for different objects on a workbench. An algorithm is presented that uses the learned dynamic model to infer the goal location of the reaching hand ahead of time. The goal location inference can be useful for path planning and collision avoidance in applications involving humanâ€“robot collaboration. The inference algorithm does not depend on the human subject or the number of objects and their placement.
