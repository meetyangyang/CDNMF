/i>&#x2014;Robotic grasping often needs to handle novel categories of objects. As a result;  frequent retraining of the classification neural network is a pain point;  which is tedious and prone to overfitting with only a few samples. In this work;  metric learning is introduced for grasping classification where a novel kind of weight imprinting classification is proposed to handle the novel classes by better feature embedding and directly setting the classifier weights without retraining or fine-tuning. Together with the benefits from the variable stiffness feature of the gripper;  the proposed vision-based force grasping approach can handle a wide range of objects from fragile to heavy;  and the grasping force is controllable from 0.2 N onward to the motor limitation. The controllable grasping force resolution of the proposed vision grasping is better than 0.05 N;  the accuracy of the grasping force is evaluated from 0.2 to 12 N;  and the evaluated grasping objects are from extremely fragile potato chips and eggshell to heavy flange and metal block. 
