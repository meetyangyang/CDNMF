We present a framework for autonomous driving which can learn from human demonstrations;  and we apply it to the longitudinal control of an autonomous car. Offline;  we model car-following strategies from a set of example driving sequences. Online;  the model is used to compute accelerations which replicate what a human driver would do in the same situation. This reference acceleration is tracked by a predictive controller which enforces a set of comfort and safety constraints before applying the final acceleration. The controller is designed to be robust to the uncertainty in the predicted motion of the preceding vehicle. In addition;  we estimate the confidence of the driver model predictions and use it in the cost function of the predictive controller. As a result;  we can handle cases where the training data used to learn the driver model does not provide sufficient information about how a human driver would handle the current driving situation. The approach is validated using a combination of simulations and experiments on our autonomous vehicle. 