Black-box optimization algorithms typically start a search from scratch;  assuming little prior knowledge about the task at hand. In practice;  this approach can be prohibitive for computationally expensive problems;  as a large number of costly function evaluations are often needed before a suitable (near-optimal) solution is found. Under this observation;  recent efforts have incorporated \n<italic xmlns:mml=\http://www.w3.org/1998/Math/MathML\ xmlns:xlink=\http://www.w3.org/1999/xlink\>transfer learning</i>\n capabilities into sequential model-based Bayesian optimization (BO) solvers;  resulting in substantial performance speed-ups by leveraging information from related past problems. However;  a common simplifying assumption in existing approaches is that the search spaces of a previously encountered \n<italic xmlns:mml=\http://www.w3.org/1998/Math/MathML\ xmlns:xlink=\http://www.w3.org/1999/xlink\>source</i>\n and the ongoing \n<italic xmlns:mml=\http://www.w3.org/1998/Math/MathML\ xmlns:xlink=\http://www.w3.org/1999/xlink\>target</i>\n task bear the same features and dimensionality;  with the difference lying in their respective objective functions. In this article;  we present a generalized \n<italic xmlns:mml=\http://www.w3.org/1998/Math/MathML\ xmlns:xlink=\http://www.w3.org/1999/xlink\>transfer BO</i>\n algorithm that relaxes the aforementioned assumption. Our method jointly transforms source features while training probabilistic transfer regression models for the target;  thus applying to practical use-cases where (in addition to the difference in objective functions) the number of features could change across the source and target tasks; for example;  features can be added and/or removed. The theoretical basis of our proposal is analyzed;  and its empirical performance is demonstrated on synthetic benchmark functions as well as in realistic examples spanning engineering design and the automated configuration of a machine learning model. 