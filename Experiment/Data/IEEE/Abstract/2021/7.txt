This article formulates gantry real-time scheduling in a gantry work cell;  where the material transfer is driven by gantries;  as a Markov decision process (MDP). Classical learning methods and planning methods for solving the optimization problems in MDP are discussed. An innovative method;  called “Q-ADP; ” is proposed to integrate reinforcement learning (RL) with approximate dynamic programming (ADP). Q-ADP uses model-free Q-learning algorithm to learn state values through interactions with the environment;  meanwhile;  planning steps during the learning process opt for ADP to keep updating state values through several sample paths. A model of one-step transition probabilities is built based on the machines' reliability model;  and serves the ADP algorithm. To demonstrate the effectiveness of this method;  a numerical study is performed to show the production performance;  compared to a standard Q-learning algorithm. The simulation results show that Q-ADP outperforms standard Q-learning under the same length of training process. It is also shown that with the benefit of repeated updating state values through sample paths;  Q-ADP requires less data for gantry policy to converge;  which makes the method promising when real data are limited. 