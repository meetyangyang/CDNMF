In this paper;  we investigate the cross-modal material retrieval problem;  which permits the user to submit a multimodal query including tactile and auditory modalities;  and retrieve the image results of visual modalities. Since multiple significantly different modalities are involved in this process;  we encounter more challenges compared with the existing cross-modal retrieval tasks. Our focus is to learn cross-modal representations when the modalities are significantly different and with minimal supervision. A novelty is that we establish a framework that deals with weakly paired multimodal fusion method for heterogenous tactile and auditory modalities and weakly paired cross-modal transfer for visual modality. A structured dictionary learning method with a low rank and common classifier is developed to obtain the modal-invariant representation. Finally;  some cross-modal validations on publicly available data sets are performed to show the advantages of the proposed method. 