The fusion of sensor data is an essential requirement for active perception in autonomous robots in order to accurately perceive an environment;  make appropriate adjustments that improve the understanding of the environment and generate actionable insights. Despite a large body of literature in this domain;  only a handful of research focuses on unsupervised machine learning;  which is increasingly important in unlabeled;  unknown environments. This paper proposes a scalable self-organizing neural architecture for environmental perception based on multimodal fusion using unsupervised machine learning. Inspired by the biological counterpart;  the neural architecture consists of topographic maps across each modality and incorporates a novel scalable self-organization process that handles high volume;  high-velocity sensor data. The co-occurrence relationships across modalities are captured in cross-modal connections and the reverse Hebbian projection is used for multimodal representation. This proposed unsupervised machine learning-based scalable fusion technique was implemented on Apache Hadoop and Apache Spark. It was validated using an extensive multimodal human activity sensor dataset to demonstrate efficient representation and multimodal fusion for active perception. 