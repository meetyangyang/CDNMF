Reinforcement learning (RL) has been increasingly used for single peg-in-hole assembly;  where assembly skill is learned through interaction with the assembly environment in a manner similar to skills employed by human beings. However;  the existing RL algorithms are difficult to apply to the multiple peg-in-hole assembly because the much more complicated assembly environment requires sufficient exploration;  resulting in a long training time and less data efficiency. To this end;  this article focuses on how to predict the assembly environment and how to use the predicted environment in assembly action control to improve the data efficiency of the RL algorithm. Specifically;  first;  the assembly environment is exactly predicted by a variable time-scale prediction (VTSP) defined as general value functions (GVFs);  reducing the unnecessary exploration. Second;  we propose a fuzzy logic-driven variable time-scale prediction-based reinforcement learning (FLDVTSP-RL) for assembly action control to improve the efficiency of the RL algorithm;  in which the predicted environment is mapped to the impedance parameter in the proposed impedance action space by a fuzzy logic system (FLS) as the action baseline. To demonstrate the effectiveness of VTSP and the data efficiency of the FLDVTSP-RL methods;  a dual peg-in-hole assembly experiment is set up; the results show that FLDVTSP-deep Q-learning (DQN) decreases the assembly time about 44% compared with DQN and FLDVTSP-deep deterministic policy gradient (DDPG) decreases the assembly time about 24% compared with DDPG.