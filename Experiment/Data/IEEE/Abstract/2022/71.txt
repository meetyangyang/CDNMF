Universal grasping for a diverse range of objects is a challenging problem in robotics;  especially in the presence of mixed properties with fragile/rigid and heavy/light. Toward universal grasping;  this article presents a practical and systematic grasping control framework that enables a variable stiffness gripper to handle the objects with diverse properties using a category-aware force regulation approach;  termed classification-based force grasping. Under this framework;  a convolutional neural network (CNN) is employed to classify the category of the grasping object;  and a grasping force is determined based on the classified category through a database that records a predefined force magnitude per category. Sequentially;  the gripper can be adjusted to a force-optimized stiffness;  which facilitates the achievement of an accurate grasping force regulation in a large range. Technically;  two novel enabling modules are developed for grasping classification and execution;  respectively. First;  a novel weight imprinting technique based on center-guided feature embedding is proposed for object classification. It enables the CNN to efficiently handle novel object categories using only a few samples even without retraining/fine-tuning. Second;  a vision-based grasping force sensing module is developed;  which takes advantage of the specifically designed variable-stiffness gripper. Its grasping force can be estimated from the deflection angle of finger flexure by the vision so that the contact force can be sensed and regulated. Remarkably;  only single-source vision information is needed for both of the above modules without any additional force sensor. Experiments are conducted extensively to evaluate the performance of the proposed force grasping approach. \n<italic xmlns:mml=\http://www.w3.org/1998/Math/MathML\ xmlns:xlink=\http://www.w3.org/1999/xlink\>