To realize a higher-level autonomy of surgical knot tying in minimally invasive surgery (MIS);  automated suture grasping;  which bridges the suture stitching and looping procedures;  is an important yet challenging task needs to be achieved. This paper presents a holistic framework with image-guided and automation techniques to robotize this operation even under complex environments. The whole task is initialized by suture segmentation;  in which we propose a novel semi-supervised learning architecture featured with a suture-aware loss to pertinently learn its slender information using both annotated and unannotated data. With successful segmentation in stereo-camera;  we develop a Sampling-based Sliding Pairing (SSP) algorithm to online optimize the suture’s 3D shape. By jointly studying the robotic configuration and the suture’s spatial characteristics;  a target function is introduced to find the optimal grasping pose of the surgical tool with Remote Center of Motion (RCM) constraints. To compensate for inherent errors and practical uncertainties;  a unified grasping strategy with a novel vision-based mechanism is introduced to autonomously accomplish this grasping task. Our framework is extensively evaluated from learning-based segmentation;  3D reconstruction;  and image-guided grasping on the da Vinci Research Kit (dVRK) platform;  where we achieve high performances and successful rates in perceptions and robotic manipulations. These results prove the feasibility of our approach in automating the suture grasping task;  and this work fills the gap between automated surgical stitching and looping;  stepping towards a higher-level of task autonomy in surgical knot tying. 