Robot Learning from humans is a promising paradigm for directly transferring human skills to robots. This learning allows robots to encapsulate task constraints and motion patterns from human demonstrations as well as acquire skills that can be adapted to unseen scenarios. Even though many state-of-the-art skill-learning successes have been achieved;  simultaneously addressing variability from a complex and long-horizon manipulation task and generalizing it to external uncertainty remains a challenge. This efficient skill learning has to allow for handling large-scale;  high-dimensional demonstrations;  adapting to environmental changes (starting;  via and end points and obstacles);  generalizing to task constraints (trajectory precision;  stiffness);  and measuring uncertainty in the reproduction. To this effect;  we present a novel robot skill-learning framework called SVGP-CoGP that will implement all the aforementioned properties by encoding task variability from multiple demonstrations using Sparse Variational Gaussian Processes (SVGP) and adapting to additional constraints via a coregionalized multi-output GP (CoGP) based on SVGP. The proposed method can significantly reduce the computational complexity of model fitting by making use of the variational inference of GP models;  which makes it possible for robots to learn skills from complex and long-horizon tasks. We evaluated and compared the effectiveness and strengths of our framework with existing probabilistic methods on a Kinova robot that performed emergency button-pressing tasks. The results indicated that our framework allowed the robot to learn skills from complex and long-horizon manipulation tasks that outperformed baselines both in quantitative evaluation and in an online test. 