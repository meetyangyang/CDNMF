Visual localization is a fundamental capability in robotics and has been well studied for recent decades. Although many state-of-the-art algorithms have been proposed;  great success usually builds on the assumption that the working environment is static. In most of the real scenes;  the assumption cannot hold because there are inevitably moving objects;  especially humans;  which significantly degrade the localization accuracy. To address this problem;  we propose a robust visual localization system building on top of a feature-based visual simultaneous localization and mapping algorithm. We design a dynamic region detection method and use it to preprocess the input frame. The detection process is achieved in a Bayesian framework which considers both the prior knowledge generated from an object detection process and observation information. After getting the detection result;  feature points extracted from only the static regions will be used for further visual localization. We performed the experiments on the public TUM data set and our recorded data set;  which shows the daily dynamic scenarios. Both qualitative and quantitative results are provided to show the feasibility and effectiveness of the proposed method. 