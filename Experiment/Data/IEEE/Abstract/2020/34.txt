To more actively perform fine manipulation tasks in the real world;  intelligent robots should be able to understand and communicate the physical attributes of the material during interaction with an object. Tactile and vision are two important sensing modalities in robotic perception system. In this article;  we propose a cross-modal material perception framework for recognizing novel objects. Concretely;  it first adopts an object-agnostic method to associate information from tactile and visual modalities. It then recognizes a novel object by using its tactile signal to retrieve perceptually similar surface material images through the learned cross-modal correlation. This problem exhibits a challenge because data from visual and tactile modalities are highly heterogeneous and weakly paired. Moreover;  the framework should not only consider cross-modal pairwise relevance but also be discriminative and generalized for unseen objects. To this end;  we propose a weakly paired cross-modal adversarial learning (WCMAL) model for the visual-tactile cross-modal retrieval;  which combines the advantages of deep learning and adversarial learning. In particular;  the model fully considers the weak pairing problem between the two modalities. Finally;  we conduct verification experiments on a publicly available data set. The results demonstrate the effectiveness of the proposed method. 