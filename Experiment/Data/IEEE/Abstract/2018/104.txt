The ever-growing development of sensor technology has led to the use of multimodal sensors to develop robotics and automation systems. It is therefore highly expected to develop methodologies capable of integrating information from multimodal sensors with the goal of improving the performance of surveillance;  diagnosis;  prediction;  and so on. However;  real multimodal data often suffer from significant weak-pairing characteristics;  i.e.;  the full pairing between data samples may not be known;  while pairing of a group of samples from one modality to a group of samples in another modality is known. In this paper;  we establish a novel projective dictionary learning framework for weakly paired multimodal data fusion. By introducing a latent pairing matrix;  we realize the simultaneous dictionary learning and the pairing matrix estimation;  and therefore improve the fusion effect. In addition;  the kernelized version and the optimization algorithms are also addressed. Extensive experimental validations on some existing data sets are performed to show the advantages of the proposed method. 