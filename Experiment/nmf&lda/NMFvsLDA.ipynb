{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e4786844",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize, pos_tag\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from tqdm import tqdm\n",
    "# import nltk\n",
    "# nltk.download('wordnet')\n",
    "\n",
    "# 获取单词的词性\n",
    "def get_wordnet_pos(tag):\n",
    "    if tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def lemmatizing(data_samples_raw):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    data_samples_cleaned=[]\n",
    "    for ind_doc in tqdm(range(0,len(data_samples_raw))):  # 使用 tqdm 显示进度百分比\n",
    "        sentence=data_samples_raw[ind_doc].lower()\n",
    "        tokens = word_tokenize(sentence)  # 分词\n",
    "        tagged_sent = pos_tag(tokens)     # 获取单词词性\n",
    "        lemmas_sent = []\n",
    "        for tag in tagged_sent:\n",
    "            wordnet_pos = get_wordnet_pos(tag[1]) or wordnet.NOUN\n",
    "            lemmas_sent.append(lemmatizer.lemmatize(tag[0], pos=wordnet_pos)) # 词形还原\n",
    "    #     data_samples_cleaned.append(lemmas_sent)\n",
    "        data_samples_cleaned.append(' '.join(lemmas_sent))\n",
    "    return(data_samples_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "da881a25",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "def find_data_directory(file_name):\n",
    "    current_directory = os.getcwd()\n",
    "    parent_directory = os.path.dirname(current_directory)\n",
    "\n",
    "    data_directory = None  # 初始化data文件夹地址为None\n",
    "\n",
    "    for file_or_dir in os.listdir(parent_directory):\n",
    "        full_path = os.path.join(parent_directory, file_or_dir)  # 获取文件或文件夹的完整路径\n",
    "        if os.path.isdir(full_path) and file_or_dir == file_name:\n",
    "            data_directory = full_path  # 如果是文件夹并且名称为\"data\"，则将其地址赋值给data_directory变量\n",
    "            break  # 找到了就退出循环\n",
    "\n",
    "    if data_directory:\n",
    "        print(\"找到了名为\",file_name,\"的文件夹，地址为：\", data_directory)\n",
    "        return data_directory\n",
    "    else:\n",
    "        print(\"找到了名为\",file_name,\"的文件夹！\")\n",
    "        return None\n",
    "# print([find_data_directory('Data')+\"\\\\IEEE\\\\Abstract\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "27804dbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset...\n",
      "找到了名为 Data 的文件夹，地址为： E:\\CDNMF\\Experiment\\Data\n",
      "done in 0.186s.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "=======================================================================================\n",
    "Topic extraction with Non-negative Matrix Factorization and Latent Dirichlet Allocation\n",
    "=======================================================================================\n",
    "\n",
    "This is an example of applying Non-negative Matrix Factorization\n",
    "and Latent Dirichlet Allocation on a corpus of documents and\n",
    "extract additive models of the topic structure of the corpus.\n",
    "The output is a list of topics, each represented as a list of terms\n",
    "(weights are not shown).\n",
    "\n",
    "The default parameters (n_samples / n_features / n_topics) should make\n",
    "the example runnable in a couple of tens of seconds. You can try to\n",
    "increase the dimensions of the problem, but be aware that the time\n",
    "complexity is polynomial in NMF. In LDA, the time complexity is\n",
    "proportional to (n_samples * iterations).\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import print_function\n",
    "from time import time\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import NMF, LatentDirichletAllocation\n",
    "# from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "# Load the 20 newsgroups dataset and vectorize it. We use a few heuristics\n",
    "# to filter out useless terms early on: the posts are stripped of headers,\n",
    "# footers and quoted replies, and common English words, words occurring in\n",
    "# only one document or in at least 95% of the documents are removed.\n",
    "\n",
    "print(\"Loading dataset...\")\n",
    "t0 = time()\n",
    "# dataset = fetch_20newsgroups(shuffle=True, random_state=1,\n",
    "#                              remove=('headers', 'footers', 'quotes'))\n",
    "# data_samples = dataset.data\n",
    "import os\n",
    "\n",
    "data_samples_raw=[]\n",
    "for dirpath, dirnames, filenames in os.walk(find_data_directory('Data')+\"\\\\IEEE\\\\Abstract\"):\n",
    "#     print('Directory', dirpath)\n",
    "    for filename in filenames:\n",
    "#         print(' File', filename)\n",
    "        try:\n",
    "            dir=os.path.join(dirpath, filename)\n",
    "            f = open(dir,'r',encoding='utf-8')    # 打开文件\n",
    "#             data = f.readline()                   # 读取文件内容\n",
    "#             print(' File', data)\n",
    "            lines = f.readlines()  # 读取所有行\n",
    "#             print(type(lines))\n",
    "            data_samples_raw.append(lines[0])  # 取第4行为abstract\n",
    "#             last_line = lines[-1]  # 取最后一行为keywords\n",
    "            \n",
    "        finally:\n",
    "            if f:\n",
    "                f.close()                     # 确保文件被关闭\n",
    "# print(' File', data_samples)\n",
    "\n",
    "\n",
    "print(\"done in %0.3fs.\" % (time() - t0))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5ab3bc09",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 2058/2058 [00:17<00:00, 116.14it/s]\n"
     ]
    }
   ],
   "source": [
    "data_samples=lemmatizing(data_samples_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4caccaa8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2058\n"
     ]
    }
   ],
   "source": [
    "n_samples = len(data_samples)\n",
    "print(n_samples)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4e1a4f19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "找到了名为 text 的文件夹，地址为： E:\\CDNMF\\Experiment\\text\n",
      "Extracting tf-idf features for NMF...\n",
      "done in 1.136s.\n",
      "Fitting the NMF model with tf-idf features,n_samples=2058 and n_features=6000...\n",
      "done in 0.959s.\n",
      "\n",
      "Topics in NMF model:\n",
      "Topic #0:\n",
      "controller power uncertainty mode output input loop simulation plant subsystem track effectiveness logic grid actuator base operation feedback delay closed loop\n",
      "Topic #1:\n",
      "robot mobile mobile robot localization environment task map multirobot robotic formation leader move robot move communication human robot task robot cycle perform robot motion skin\n",
      "Topic #2:\n",
      "1998 1998 1999 1999 italic italic 1998 1999 italic 1998 1999 1998 1999 1998 1999 1998 1999 1998 article formulatype formulatype 1998 formulatype 1998 1999 observation para notion game notion 1998 1999 notion 1998 norm\n",
      "Topic #3:\n",
      "wafer tool cluster tool cluster schedule cycle chamber lot wafer delay delay armed processing wafer lot armed cluster armed cluster tool cyclic manufacturing backward variation wafer fabrication\n",
      "Topic #4:\n",
      "service composition web web service service composition user provider cloud customer compatibility web service composition mobile issue environment business need reputation service service requirement service provider\n",
      "Topic #5:\n",
      "production machine line job consumption manufacturing operation batch schedule production line shop bernoulli reduce setup part evaluate evaluation processing saving line machine\n",
      "Topic #6:\n",
      "net petri net petri transition marking deadlock unobservable pn place supervisor timed diagnosability pns resource token behavior fire unobservable transition siphon fm\n",
      "Topic #7:\n",
      "cell injection optical manipulation force feedback adherent adherent cell vision tip microinjection measurement cell injection tweezer transport optical tweezer microfluidic robotic cell temperature micropipette\n",
      "Topic #8:\n",
      "tag rfid identification radio frequency radio frequency frequency identification radio frequency identification identification rfid frequency identification rfid rfid tag frame slot aloha phase uhf estimate localization dfsa anticollision\n",
      "Topic #9:\n",
      "path vehicle planning path planning plan travel rout ugv autonomous routing delivery surface route along obstacle target simulation rrt length vehicle rout\n",
      "Topic #10:\n",
      "image feature defect detection surface classification accuracy camera sample high noise base classifier pattern map experiment shape compare color signal\n",
      "Topic #11:\n",
      "sensor degradation measurement fusion failure signal estimation estimate unit filter sensor signal prognostic methodology node target index reliability predict component localization\n",
      "Topic #12:\n",
      "object force grasp contact manipulation grasping hand finger gripper robotic position contact force arm surface task workpiece vision configuration cage shape\n",
      "Topic #13:\n",
      "task cloud schedule resource agent assignment workflow allocation provider communication profit task allocation minimize execution schedule task deadline user perform compute operation\n",
      "Topic #14:\n",
      "assembly product disassembly part component manufacturing fixture variation tolerance methodology skill shape material module eol dimensional assembly skill line generate complex\n",
      "Topic #15:\n",
      "policy inventory customer supply chain threshold allocation care supply chain resource action budget markov policy policy referral simulation stage maximize price queue\n",
      "Topic #16:\n",
      "human activity interaction human robot user learn intention task recognition subject robot skill hand human activity safety assembly gait human intention exoskeleton weld\n",
      "Topic #17:\n",
      "trajectory motion planning track motion planning manipulator joint trajectory planning plan tracking position velocity target obstacle gait nanowires generate exoskeleton rrt workspace\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "n_samples = len(data_samples)\n",
    "n_features = 6000\n",
    "n_topics = 18\n",
    "n_top_words = 20\n",
    "def load_stopwords( inpath = find_data_directory('text')+\"\\\\stopwords_yy20220521.txt\" ):\n",
    "\t\"\"\"\n",
    "\tLoad stopwords from a file into a set.\n",
    "\t\"\"\"\n",
    "\tstopwords = set()\n",
    "\twith open(inpath, 'r', encoding='utf-8') as f:\n",
    "\t\tlines = f.readlines()\n",
    "\t\tfor l in lines:\n",
    "\t\t\tl = l.strip()\n",
    "\t\t\tif len(l) > 0:\n",
    "\t\t\t\tstopwords.add(l)\n",
    "\treturn stopwords\n",
    "\n",
    "def print_top_words(model, feature_names, n_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print(\"Topic #%d:\" % topic_idx)\n",
    "        print(\" \".join([feature_names[i]\n",
    "        for i in topic.argsort()[:-n_top_words - 1:-1]]))\n",
    "    print()\n",
    "\n",
    "# Use tf-idf features for NMF.\n",
    "print(\"Extracting tf-idf features for NMF...\")\n",
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.95, min_df=2, #max_features=n_features,\n",
    "                                   stop_words=load_stopwords(), ngram_range = (1,3))\n",
    "t0 = time()\n",
    "tfidf = tfidf_vectorizer.fit_transform(data_samples)\n",
    "print(\"done in %0.3fs.\" % (time() - t0))\n",
    "\n",
    "# Fit the NMF model\n",
    "print(\"Fitting the NMF model with tf-idf features,\"\n",
    "      \"n_samples=%d and n_features=%d...\"\n",
    "      % (n_samples, n_features))\n",
    "t0 = time()\n",
    "nmf = NMF(init='nndsvda', n_components=n_topics, random_state=0, alpha_W=0,alpha_H=0, l1_ratio=1).fit(tfidf)\n",
    "print(\"done in %0.3fs.\" % (time() - t0))\n",
    "print(\"\\nTopics in NMF model:\")\n",
    "tfidf_feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "print_top_words(nmf, tfidf_feature_names, n_top_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a4f22fc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting tf features for LDA...\n",
      "done in 0.768s.\n",
      "Fitting LDA models with tf features, n_samples=2058 and n_features=6000...\n",
      "done in 2.535s.\n",
      "\n",
      "Topics in LDA model:\n",
      "Topic #0:\n",
      "calibration camera image afm chiller measurement scan drift nanostructure vo imu instrument scene helical band odometry deflection neck monocular microswimmers\n",
      "Topic #1:\n",
      "leader agent game follower auction formation mechanism slip parking rotation price player locomotion homogeneous sand procurement vcg bipedal volume phase\n",
      "Topic #2:\n",
      "policy schedule disassembly price service cloud customer rbc eol job minimize inventory base electricity park provider formulate spike makespan charge\n",
      "Topic #3:\n",
      "phase volume micro successful cubic perturbation able highly meso delivery robotic cell perishable intervention supply transport automatic produce protein constructive\n",
      "Topic #4:\n",
      "grasp hand gait object grasping exoskeleton walk finger tolerance suture manipulation robotic limb rehabilitation hip assistance cage contact walking force\n",
      "Topic #5:\n",
      "supply chain iga pevs dbn service compression web grid garment tl mesh v2g management delivery aggregator offer prosumers open accelerator\n",
      "Topic #6:\n",
      "cell dt injection force diagnosability registration feedback larva zebrafish technology twin success microinjection biological touch embryo conveyor vision tip dtim\n",
      "Topic #7:\n",
      "1999 1998 vehicle italic transport traffic xref ordered rout formulatype ref notion path minimize open whether travel restriction motion dynamically\n",
      "Topic #8:\n",
      "fixture workpiece contact knot rod printer dna droplet situ tighten clamp diameter tying force fixturing locator tightening accessibility rom synthesis\n",
      "Topic #9:\n",
      "nn snm mr usvs qc ws quantile simplex conjecture solo multistation vm servomotors nelder mead incorporates semiglobally sguub simply nanotube\n",
      "Topic #10:\n",
      "robot base task simulation sensor human experiment high article environment compare achieve motion exist italic path challenge detection 1999 1998\n",
      "Topic #11:\n",
      "production machine tool product assembly manufacturing wafer line part controller schedule base operation cycle stage reduce cluster consumption variation high\n",
      "Topic #12:\n",
      "food foodservice explainable drink ehil author imitation fbm logical sae pouring pour solid equivalence covered covering manipulability branching explainability clone\n",
      "Topic #13:\n",
      "image feature surface defect classification detection training curve classifier generate noise neural accuracy tactile convolutional recognition autoencoder cnn pattern normal\n",
      "Topic #14:\n",
      "dropout packet tap rmpc uwb token circuit transponder favorable stepping fir kf center nc adjust ultrawideband negative flotation lock simultaneous\n",
      "Topic #15:\n",
      "wmrs jaw cylindrical duct wheel unilateral part secondary fixture side biaxial horizontally groove concavity actuating phase applies screw primary pair\n",
      "Topic #16:\n",
      "color cf pns tft replication net lcd mf roadmap valued afm match vm token complex crystal extension spsa intuitionistic plate\n",
      "Topic #17:\n",
      "anomaly risk 19 covid occur population adl hydraulic recurrent elderly collection advisory apartment spike sensor dp major supply extremely abnormal\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "# Use tf (raw term count) features for LDA.\n",
    "print(\"Extracting tf features for LDA...\")\n",
    "tf_vectorizer = CountVectorizer(max_df=0.95, min_df=2, max_features=n_features,\n",
    "                                stop_words=load_stopwords())\n",
    "t0 = time()\n",
    "tf = tf_vectorizer.fit_transform(data_samples)\n",
    "print(\"done in %0.3fs.\" % (time() - t0))\n",
    "\n",
    "print(\"Fitting LDA models with tf features, n_samples=%d and n_features=%d...\"\n",
    "      % (n_samples, n_features))\n",
    "lda = LatentDirichletAllocation(n_components=n_topics, max_iter=5,\n",
    "                                learning_method='online', learning_offset=50.,\n",
    "                                random_state=0)\n",
    "t0 = time()\n",
    "lda.fit(tf)\n",
    "print(\"done in %0.3fs.\" % (time() - t0))\n",
    "\n",
    "print(\"\\nTopics in LDA model:\")\n",
    "tf_feature_names = tf_vectorizer.get_feature_names_out()\n",
    "print_top_words(lda, tf_feature_names, n_top_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "43236df4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "当前工作目录的根目录为： E:\\\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'E:\\\\'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "def find_root_directory():\n",
    "    current_directory = os.getcwd()\n",
    "    root_directory = current_directory\n",
    "\n",
    "    while os.path.dirname(root_directory) != root_directory:\n",
    "        root_directory = os.path.dirname(root_directory)\n",
    "\n",
    "    print(\"当前工作目录的根目录为：\", root_directory)\n",
    "    return root_directory\n",
    "find_root_directory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ace9434",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
